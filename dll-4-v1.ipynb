{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7323999,"sourceType":"datasetVersion","datasetId":4250696}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''\nTemplate for the 4th assignment\nStudent: NAME SURNAME\n'''\n\n############################\n# Packages\n############################\nimport torch\nimport torch.nn as nn\nimport math\nimport regex as re\nimport spacy\nfrom tqdm import tqdm\nimport pickle\nimport os\n\n############################\n#Methods\n###########################\n\n# ________________________________FILE_SYSTEM___________________________________\n\ndef exists(directory_path, file_name):\n    file_path = os.path.join(directory_path, file_name)\n\n    # Check if the file exists\n    if os.path.exists(file_path):\n        # Check if the file is not empty\n        if os.path.getsize(file_path) > 0:\n            # Assuming it's a pickle file, you can add additional checks if needed\n            if ((file_name.endswith('.pkl')) or (file_name.endswith('.pickle'))):\n                return True\n            else:\n                print(f\"The file '{file_name}' is not a pickle file.\")\n        else:\n            print(f\"The file '{file_name}' is empty.\")\n    else:\n        print(f\"The file '{file_name}' does not exist in the specified directory.\")\n\n    return False\n\n# ________________________________TOKENIZATION___________________________________\n\ndef tokenize_with_spacy(sentence_pairs, nlp):\n    tokenized_pairs = []\n\n    # Apply custom preprocessing rules\n    # Include specific punctuation (?, !, ., ,, -) and alphanumeric characters\n    allowed_characters = re.escape(\"?.!,-'\")\n    \n    \n    for question, answer in tqdm(sentence_pairs, desc='Tokenizing'):\n        # Process the question and answer sentences\n        question = re.sub(r'\\s+', ' ', re.sub(rf'[^a-zA-Z0-9{allowed_characters}\\s]', '', \n                                       re.sub(r'--+', ' ', question)))\n\n        answer = re.sub(r'\\s+', ' ', re.sub(rf'[^a-zA-Z0-9{allowed_characters}\\s]', '', \n                                     re.sub(r'--+', ' ', answer)))\n        \n        question_doc = nlp(question)\n        answer_doc = nlp(answer)\n        \n        # Tokenize and append <EOS> and <SOS>\n        tokenized_question = [token.text for token in question_doc] + [\"<EOS>\"]\n        tokenized_answer = [\"<SOS>\"] + [token.text for token in answer_doc] + [\"<EOS>\"]\n        \n        # Add the tokenized pair to the list\n        tokenized_pairs.append((tokenized_question, tokenized_answer))\n    \n    return tokenized_pairs\n\n\ndef clear_punctuation(s):\n     # Lowercase the text\n    s = s.lower()\n\n    # Correctly handle hyphenated words: replace hyphens with a placeholder when part of a word\n    s = re.sub(r'(?<=\\w)-(?=\\w)', 'HYPHENPLACEHOLDER', s)\n\n    # Keep contractions intact: look for apostrophes followed by common contraction endings and prevent space insertion\n    contraction_patterns = [r\"(?<=\\w)'(re|ve|ll|t|s|d|m|n't)\"]\n    for pattern in contraction_patterns:\n        s = re.sub(pattern, r'HYPHENAPOSTROPHE\\1', s)\n\n    # Separate punctuation (.,!?) from words\n    s = re.sub(r'([,.!?])', r' \\1 ', s)\n\n    # Remove all non-word characters except apostrophes, placeholders, spaces, and punctuation\n    s = re.sub(r'[^\\w\\sHYPHENPLACEHOLDERHYPHENAPOSTROPHE,.!?]', '', s)\n\n    # Replace placeholders back with their original characters\n    s = s.replace('HYPHENPLACEHOLDER', '-')\n    s = s.replace('HYPHENAPOSTROPHE', \"'\")\n\n    # Replace multiple spaces with a single space\n    s = re.sub(r'\\s+', ' ', s)\n\n    # Trim leading and trailing spaces\n    s = s.strip()\n\n    return s\n\n# ________________________________DOWNLOADs___________________________________\n\ndef read_file(file_path):\n        encodings = ['utf-8', 'latin-1', 'windows-1252']\n        for encoding in encodings:\n            try:\n                with open(file_path, 'r', encoding=encoding) as file:\n                    return file.readlines()\n            except UnicodeDecodeError:\n                continue\n        raise ValueError(f\"Error: File {file_path} not readable with utf-8, latin-1, or windows-1252 encodings.\")\n\ndef print2sentences(conversations_path, lines_path):\n\n    conversations = read_file(conversations_path)\n    lines = read_file(lines_path)\n\n    print(\"First 2 conversations:  \\n\", \"1. \", conversations[0], \"\\n 2. \", conversations[1])\n    print(\"First 2 lines:  \\n\", \"1. \", lines[0], \"\\n 2. \", lines[1])\n\n\n# ________________________________TUPLE-IZATION___________________________________\n\ndef tokenize_sentence_pairs(sentence_pairs):\n    def tokenize(sentence):\n        sentence = clear_punctuation(sentence)\n        return sentence.strip().split()\n\n    tokenized_pairs = []\n    for question, answer in sentence_pairs:\n        tokenized_question = tokenize(question) + [\"<EOS>\"]\n        tokenized_answer = [\"<SOS>\"] + tokenize(answer) + [\"<EOS>\"]\n        tokenized_pairs.append((tokenized_question, tokenized_answer))\n\n    return tokenized_pairs\n\n# ________________________________PLOTS___________________________________\n\ndef plot_distribution(sentence_lenght,toprint=True, label='', title='Plot of Sentence Lenght Distribution', ylabel='Frequency', xlabel='Lenght(#tokens)'):\n    if (toprint == True):\n        plt.figure(figsize=(10, 6))\n        plt.hist(sentence_lengths, bins=30)\n        plt.xlabel(xlabel)\n        plt.ylabel(ylabel)\n        plt.title(title)\n        plt.legend()\n        plt.grid(True)\n        plt.show()\n    else:\n        return plt.hist(sentence_lengths, bins=30)\n\n\n\n############################\n# Classes\n############################\n# Vocabulary class\nclass Vocabulary:\n    '''\n    Class for dealing with our corpus\n    '''\n\n    def __init__(self, name, pairs):\n        \"\"\"\n        Args:\n            name (str): name of the language\n            pairs (list): list of pairs of sentences\n        \"\"\"\n        self.name = name\n        self.word2index = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2}\n        self.index2word = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\"}\n        self.pairs = pairs\n\n    def add_word(self, word):\n        '''\n        Add a word to the vocabulary\n        :param word: a string\n        '''\n        # TODO: add the word to the vocabulary\n        pass\n\n    def add_sentence(self, sentence):\n        '''\n        Add a sentence to the vocabulary\n        :param sentence: list of strings (words)\n        '''\n        # TODO add the sentence to the vocabulary, this method will call the add_word method\n        pass\n    \n\n# Dataset class\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, vocabulary, pairs):\n        # TODO We want vocabulary and pairs to be attributes of the class\n        pass\n\n    def __len__(self):\n        # TODO how many pairs do we have?\n        pass\n\n    def __getitem__(self, ix):\n        # TODO returns two tensors (question, answer) of the pair at index ix\n        # TODO the tensors should be of type torch.tensor and should contain integers (word indices)\n        pass\n\nclass PositionalEncoding(nn.Module):\n    '''\n    Adapted from\n    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n    '''\n    def __init__(self, d_model, dropout=0.0, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        self.max_len = max_len\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float()\n                             * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        try:\n            assert x.size(0) < self.max_len\n        except:\n            print(\"The length of the sequence is bigger than the max_len of the positional encoding. Increase the max_len or provide a shorter sequence.\")\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\nclass TransformerModel(nn.Module):\n    def __init__(self, vocab_size, d_model=512, pad_id=0, encoder_layers=6, decoder_layers=6, dim_feedforward=2048, num_heads=8, dropout_p=0.1):\n        super().__init__()\n        # TODO add an embedding layer\n        # TODO add a positional encoding layer\n        # TODO add a transformer layer, you can use nn.Transformer. You can use the default values for the parameters, but what about batch_first?\n        # TODO add a linear layer. Note: output should be probability distribution over the vocabulary\n\n\n        # Stuff you may need\n        self.vocab_size = vocab_size\n        self.pad_id = pad_id\n        self.num_heads = num_heads\n\n    def create_padding_mask(self, x, pad_id=0):\n        # TODO create a boolean mask for the <PAD> tokens\n        pass\n\n\n    def forward(self, src, tgt):\n        # S is the source sequence length, T is the target sequence length, N is the batch size, E is the feature number\n        # src: (N, S)\n        # tgt: (N, T)\n        # src_pad_mask: (N, S)\n        # tgt_pad_mask: (N, T)\n        # mask the future : (N * num_heads, T, T)\n\n        src_pad_mask = self.create_padding_mask(src, self.pad_id) # (N, S)\n        tgt_pad_mask = self.create_padding_mask(tgt, self.pad_id) # (N, T)\n\n        src = self.embedding(src)\n        tgt = self.embedding(tgt)\n\n        src = self.pos_encoder(src)  # (N, S, E)\n        tgt = self.pos_encoder(tgt) # (N, T, E)\n\n        # Mask the memory\n        memory_key_padding_mask = src_pad_mask  # (N, S)\n\n        # Mask the future\n        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.size(1), dtype=torch.bool).to(tgt.device) # (T, T)\n        # Expand to make it N * num_heads, T, T\n        tgt_mask = tgt_mask.unsqueeze(0).repeat(tgt.size(0) * self.num_heads, 1, 1) # (N, T, T)\n        # Transformer\n        output = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask,\n                                  tgt_key_padding_mask=tgt_pad_mask, memory_key_padding_mask=memory_key_padding_mask) # (N, T, E)\n        # Linear layer\n        output = self.linear(output) # (N, T, V)\n        return output\n\n############################\n# Methods\n############################\n\n#--------------------------------------------------------------------------\ndef create_sentence_pairs(conversations_path, lines_path):\n\n    def get_line_id_to_text(lines):\n        line_id_to_text = {}\n        for line in lines:\n            parts = line.strip().split(' +++$+++ ')\n            if len(parts) == 5:\n                line_id, _, _, _, text = parts\n                line_id_to_text[line_id] = text\n        return line_id_to_text\n\n    def generate_pairs(conversations, line_id_to_text):\n        pairs = []\n        for conversation in conversations:\n            line_ids_str = conversation.strip().split(' +++$+++ ')[-1]\n            line_ids = eval(line_ids_str)\n            conv_texts = [line_id_to_text[line_id] for line_id in line_ids if line_id in line_id_to_text]\n\n            for i in range(len(conv_texts) - 1):\n                pairs.append((conv_texts[i], conv_texts[i+1]))\n        return pairs\n\n    conversations = read_file(conversations_path)\n    lines = read_file(lines_path)\n\n    line_id_to_text = get_line_id_to_text(lines)\n    pairs = generate_pairs(conversations, line_id_to_text)\n\n    return pairs\n#--------------------------------------------------------------------------\n\n###########################\n# Main\n###########################\n\nif __name__ == \"__main__\":\n    # !!! Don't change the seed !!!\n    torch.manual_seed(42)\n    # !!!!!!\n    \n    # Download the data---------------------------------------------------------\n    conversations_path = '/kaggle/input/conversations/movie_conversations.txt'\n    lines_path = '/kaggle/input/conversations/movie_lines.txt'\n    print2sentences(conversations_path, lines_path)\n    \n    # Create the pairs----------------------------------------------------------\n    sentence_pairs = create_sentence_pairs(conversations_path, lines_path)\n    #for pair in sentence_pairs[:5]:\n        # print(pair)\n    \n    # Tokenize the data---------------------------------------------------------\n    if ((exists(\"/kaggle/working/\", \"tokenized_sentence_pairs.pickle\") == False)): #or (==False)\n    \n        \n        #tokenized_sentence_pairs = tokenize_sentence_pairs(sentence_pairs)\n        nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"])\n        print(\"[en_core_web_sm] model downloaded! \\n\")\n        tokenized_sentence_pairs = tokenize_with_spacy(sentence_pairs, nlp) \n        \n        for pair in tokenized_sentence_pairs[:5]:\n            print(\"tokenized pair -------------------------- \\n\", pair, \"\\n-----------------------------------------\")\n\n        with open('/kaggle/working/tokenized_sentence_pairs.pickle', 'wb') as handle:\n            pickle.dump(tokenized_sentence_pairs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n            \n    else:\n        print(\"=====Downloading Existing Data======\\n -----TOKENIZED SENTENCE PAIRS-----\")\n\n        with open('/kaggle/working/tokenized_sentence_pairs.pickle', 'rb') as handle:\n            tokenized_sentence_pairs = pickle.load(handle)\n        \n        for pair in tokenized_sentence_pairs[:5]:\n            print(\"tokenized pair -------------------------- \\n\", pair, \"\\n-----------------------------------------\")\n    \n    #Filter Sentences-----------------------------------------------------------\n    \n    #Plot Distribution\n    all_sentences = [sentence for pair in tokenized_sentence_pairs for sentence in pair]\n    sentence_lengths = [len(sentence) for sentence in all_sentences]\n    plot_distribution(sentence_lenghts, toPrint=True)\n    max_lenght= 20\n    \n    # Filter out the sentences that are too long\n    if ((exists(\"/kaggle/working/\", \"filtered_pairs.pickle\") == False)): #or (==False)\n        filtered_pairs = []\n        for pair in tokenized_sentence_pairs:\n            if all(len(sentence) <= max_length for sentence in pair):\n                filtered_pairs.append(pair)\n        \n        with open('/kaggle/working/filtered_pairs.pickle', 'wb') as handle:\n            pickle.dump(filtered_pairs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    else:\n        print(\"=====Downloading Existing Data======\\n -----FILTERED SENTENCE PAIRS-----\")\n\n        with open('/kaggle/working/filtered_pairs.pickle', 'rb') as handle:\n            filtered_pairs = pickle.load(handle)\n    \n    # Filter out the words that are too rare\n\n    # SAVE and put the code above into a function that you will call if you need to generate something slightly different\n\n    # Training loop (Consider writing a function for this/two separate functions for training and validation)\n\n    # Evaluation by feeding the model with one input sentence at a time\n\n    pass","metadata":{"_uuid":"76fd3897-5641-4f30-befc-da87d0cb85b8","_cell_guid":"5fe929c0-bca0-4818-82ac-ba77cbefcaac","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-03T11:22:39.089487Z","iopub.execute_input":"2024-01-03T11:22:39.089914Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"First 2 conversations:  \n 1.  u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\n \n 2.  u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\n\nFirst 2 lines:  \n 1.  L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n \n 2.  L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\n\nThe file 'tokenized_sentence_pairs.pickle' does not exist in the specified directory.\n[en_core_web_sm] model downloaded! \n\n","output_type":"stream"},{"name":"stderr","text":"Tokenizing:  50%|█████     | 110778/221367 [14:08<14:06, 130.71it/s]","output_type":"stream"}]}]}